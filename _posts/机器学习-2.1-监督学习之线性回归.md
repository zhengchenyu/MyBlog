---
title: 机器学习 2.1 监督学习之线性回归
date: 2017-06-10 21:52:17
tags:
---

<script type="text/javascript" src="/Users/zcy/Desktop/study/git/MathJax-master/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

# 监督学习

![监督学习说明](http://ord3iii9m.bkt.clouddn.com/image/github/mlearning%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E8%AF%B4%E6%98%8E.png) 

如上图所示，监督学习实质就是: 在给定训练集合，使用某种学习算法得到学习函数h，能够较为准确的预测y。

## 1 线性回归
下面有一组房价关于房子面积以及卧室数目的样本数据。试着从这些样本数据中构造线性模型，以预测房价。

| 房间数目| 房间大小 | 房价    |
| ------ |-------  | ----- |
| 2      | 88.0    |1760288 |
| 2      | 88.0    |1762136 |
| ...    | ...     |    ... |

### 1.1 梯度下降法 

#### 1.1.1 梯度下降法说明

假设房价与房间数目、房屋带下呈线性关系，有如下关系:

{% raw %}
$${h_\theta }(x) = {\theta _0} + {\theta _1}{x _1} + {\theta _2}{x _2}$$
{% endraw %}

假设\\({x_0} = 1\\),则有如下公式:

{% raw %}
$${h_\theta }(x) = {\theta _0}{x _0} + {\theta _1}{x _1} + {\theta _2}{x _2} = \sum\limits _{i = 0}^n{\theta _i}{x _i}$$
{% endraw %}

假设y为样本实际的值，因此有如下损失函数可以定义：

{% raw %}
$$J(\theta ) = \frac{1}{2}\sum\limits _{i = 0}^m {{{({h _\theta }({x^i}) - {y^i})}^2}} $$
{% endraw %}

我们只要保证上面的公式趋于0就是合适的，因此这个问题就转化为得到合适的使得\\(J(\theta )\\)最小。实际也就转化为利用最小二乘法进行回归分析。

做这样一个想想，对于二维的情况。\\(J(\theta )\\)是一个关于的二维函数的话，他一定类似于锅状的凸函数。根据梯度下降算法(参考<<最优化方法>>)，沿着某一个方向的负梯度方向就是在该方向上下降最大的方向。因此，我们可以选取各个维度的负梯度作为下降的方向，为了方便各个梯度方向选取同样的步长(尽管选取该方案不是梯度下降最快的方案，由于函数的连续性，该方向与最优方向一定非常接近)。因此，我们对任意维度采用如下公式进行递归(为步长)：

{% raw %}
$${\theta _j}: = {\theta _j} - \alpha \frac{{\partial J(\theta )}}{{\partial {\theta _j}}} = {\theta _j} - \alpha \sum\limits _{i = 0}^m{({h _\theta }({x^i}) - {y^i}) {x _j}}$$
{% endraw %}

我们简化，我们仅仅对房间大小与房价的一维关系进行回归分析。
> 注：对于每一次迭代运算都需要对所有全量信息进行计算。这样带来了大量的计算。工程上可以采用采样的方式计算新的，这里数据较少，就不用该方法了。

#### 1.1.2 梯度下降法案例
我们有一组数据，为房间大小与房价的关系。我们的目标是通过使用梯度下降算法，模拟得到满足学习函数，以预测房价。可以看如下点图:

![房价点图](http://ord3iii9m.bkt.clouddn.com/image/github/mlearning%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0-%E6%88%BF%E4%BB%B7%E7%82%B9%E5%9B%BE.png)

根据上一节的公式编写如下代码进行回归分析:

```
% 以下为主算法流程
% y = xita0 +xita1*x
mfilename='/Users/zcy/Desktop/mlearning/prices'
[v,x,y]=textread(mfilename,'%n%n%n');
 
limit = 1000;
y=y./10000;
% step
a = 0.0000001;
 
xita0 = 0.01;
xita1 = 1.6;
err = JFun(x,y,xita0,xita1);
disp(['err = ',num2str(err)])
count = 0; 
while err>limit 
    xita0 = xita0 - a*gradient(x,y,xita0,xita1,true);
    xita1 = xita1 - a*gradient(x,y,xita0,xita1,false)    
    err = JFun(x,y,xita0,xita1);
    count = count + 1;
    disp(['err = ',num2str(err),', count = ',num2str(count)])
end
 
scatter(x,y,'k')
hold on
x1 = 70:0.01:100;
y1 = xita0+x1.*xita1;
plot(x1,y1)

```

JFun.m文件流程:

```
function [s]  = JFun(x,y,xita0,xita1)
s = 0;
for i = 1:size(x)
   s = s + 0.5*((xita0*1+xita1*x(i)-y(i))^2);
end
```

gradient.m文件: 

```
function [s]  = gradient(x,y,xita0,xita1,b)
s = 0;
if b 
    for i = 1:size(x)
        s = s + (xita0*1+xita1*x(i)-y(i))*1;
    end
else 
    for i = 1:size(x)
        s = s + (xita0*1+xita1*x(i)-y(i))*x(i);
    end
end
```
> 这里为了示意，代码仅考虑两维，实际不建议代码这样写。另外，选取合适的limit值和步长需要经过打印调试。选取不当，容易造成函数无法收敛。

回归得到如下曲线：

![回归曲线1](http://ord3iii9m.bkt.clouddn.com/image/github/mlearning%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0-%E6%88%BF%E4%BB%B7%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E7%BB%93%E6%9E%9C1.png)

修改limit为500之后，回归更加准确。

![回归曲线2](http://ord3iii9m.bkt.clouddn.com/image/github/mlearning%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0-%E6%88%BF%E4%BB%B7%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E7%BB%93%E6%9E%9C2.png)

### 1.2 线性回归的解析解

略，详见斯坦福大学机器学习讲义。

### 1.3 线性回归的概率解释

本节从概率的角度进行线性回归分析。对我们的任意一组变量有如下公式：

{% raw %}
$${y^i} = {\theta ^T}{x^i} + {\varepsilon ^i}$$
{% endraw %}

其中\\(\varepsilon \\)为误差，假设我们的误差服从正太分布
\\({\varepsilon ^i} \sim {\rm N}(0,{\sigma ^2})\\)。即有如下公式：

{% raw %}
$$p({\varepsilon ^i}) = \frac{1}{{\sqrt {2\pi } \sigma }}{e^{( - \frac{{{{({\varepsilon ^i})}^2}}}{{2{\sigma ^2}}})}}$$
{% endraw %}

代入上面的公式有：

{% raw %}
$$p({y^i}|{x^i},\theta ) = \frac{1}{{\sqrt {2\pi } \sigma }}{e^{( - \frac{{{{({y^i} - {\theta ^T}{x^i})}^2}}}{{2{\sigma ^2}}})}}$$
{% endraw %}

该公式的含义为在已知\\(x ^i \\)和\\(\theta \\)的情况下，得到准确的\\(y ^i \\)概率。我们可以利用最大似然估计，最大释然估计实质就是对所有采样值调整参数得到最准确y值的方法。(详细请看概率论)。有如下公式:

{% raw %}
$$L(\theta ) = \prod\limits _{i = 1}^m {p({y^i}|{x^i},\theta )}  = \prod\limits _{i = 1}^m {\frac{1}{{\sqrt {2\pi } \sigma }}{e^{( - \frac{{{{({y^i} - {\theta ^T}{x^i})}^2}}}{{2{\sigma ^2}}})}}} $$
{% endraw %}

上面的公式是对各个数据采样得到准确y的概率的乘积，问题也就转化为调整使的上式最大，进一步转化为求的上式的倒数为0的情况。对上式去log，如下:

{% raw %}
$$[\log (L(\theta )) = \log (\prod\limits _{i = 1}^m {\frac{1}{{\sqrt {2\pi } \sigma }}{e^{( - \frac{{{{({y^i} - {\theta ^T}{x^i})}^2}}}{{2{\sigma ^2}}})}}} )$$
{% endraw %}

{% raw %}
$$\log (L(\theta )) = \sum\limits _{i = 1}^m {\log \frac{1}{{\sqrt {2\pi } \sigma }}{e^{( - \frac{{{{({y^i} - {\theta ^T}{x^i})}^2}}}{{2{\sigma ^2}}})}}}  = m\log \frac{1}{{\sqrt {2\pi } \sigma }} - \frac{1}{{{\sigma ^2}}} \cdot \frac{1}{2}\sum\limits _{i = 1}^m {{{({y^i} - {\theta ^T}{x^i})}^2}} $$
{% endraw %}

看上式最后一项，前面部分为常数，因此问题有转化为求最小值的问题了。

**因此从概率角度上考虑误差的情况下，我们前面基于最小二乘的算法也是正确的。**


### 1.4 局部加权回归(Loess)

#### 1.4.1 局部加权回归说明 
有一组非线性去先，譬如\\(y = {x^2}\\)。如果使用之前的方法拟合(即假设其为y=kx+b型曲线)，必然不会得到理想的结果。对于这样的问题，我们想求出\\(f(20)\\),加入我们只对\\(x\\)在20附近的样本进行一维拟合，我们就可以得到一个精确值。因此，我们引入如下公式：

{% raw %}
$$J(\theta ) = \frac{1}{2}\sum\limits_ {i = 0}^m {{\omega ^i}{{({h_ \theta }({x^i}) - {y^i})}^2}} $$
{% endraw %}

{% raw %}
$${\omega ^i} = {e^{ - \frac{{{{({x^i} - x)}^2}}}{{2{\tau ^2}}}}}$$
{% endraw %}

其中，在原来的价值函数中引入权重\\(\omega\\)。\\(\tau\\)是波长，该值越小，越趋于向20附近的值进行回归计算。对于求\\(f(20)\\)的情况，在的公式中\\(x\\)恒为20,可以知道该值在\\(x ^i\\)趋近于20的时候趋近于1，趋近于\\( \pm \infty \\)的时候趋近于0。也就是相当于仅仅是在20附近进行拟合。由于新引入的权重函数与\\(\theta\\)无关。所以得到如下公式:

{% raw %}
$${\theta _j}: = {\theta _j} - \alpha \frac{{\partial J(\theta )}}{{\partial {\theta _j}}} = {\theta _j} - \alpha \sum\limits _{i = 0}^m {{\omega ^i}({h _\theta }({x^i}) - {y^i})} {x _j}$$
{% endraw %}

我们这里是一个一维的问题，可以得到如下公式:

{% raw %}
$$J(\theta ) = \frac{1}{2}\sum\limits _{i = 0}^m {{\omega ^i}{{({\theta _1}{x^i} + {\theta _0} - {y^i})}^2}} $$
{% endraw %}

{% raw %}
$${\omega ^i} = {e^{ - \frac{{{{({x^i} - 20)}^2}}}{{2{\tau ^2}}}}}$$
{% endraw %}

#### 1.4.2 局部加权回归案例
主流程代码:

```
% y = xita0 +xita1*x
x = -100:1:100;
y = x.*x;
% x= 20, y = 40x - 400
x0 = 20;       
tao = 5;
a = 0.0003;
 
xita0 = -1;
xita1 = 1;
limit = 7850;
err = JFun_Loess(x,y,xita0,xita1,x0,tao);
count = 0; 
while err>limit 
    xita0 = xita0 - a*gradient_Loess(x,y,xita0,xita1,true,x0,tao);
    xita1 = xita1 - a*gradient_Loess(x,y,xita0,xita1,false,x0,tao);    
    err = JFun_Loess(x,y,xita0,xita1,x0,tao);
    count = count + 1;
end
 
scatter(x,y,'k')
hold on
x1 = -100:1:100;
y1 = xita0+x1.*xita1;
disp(['xita0 = ',num2str(xita0),', xita1 = ',num2str(xita1)])
plot(x1,y1)
```

JFun_loess:

```
function [s]  = JFun_Loess(x,y,xita0,xita1,x0,tao)
s = 0;
for i = 1:size(x')
    s = s + 0.5*((xita0*1+xita1*x(i)-y(i))^2)*exp(-0.5*(x(i)-x0)*(x(i)-x0)/tao/tao);
end
```

gradient_Loess:

```
function [s]  = gradient_Loess(x,y,xita0,xita1,b,x0,tao)
s = 0;
if b 
    for i = 1:size(x')
        s = s + (xita0*1+xita1*x(i)-y(i))*1*exp(-0.5*(x(i)-x0)*(x(i)-x0)/tao/tao);
    end
else
    for i = 1:size(x')
        s = s + (xita0*1+xita1*x(i)-y(i))*x(i)*exp(-0.5*(x(i)-x0)*(x(i)-x0)/tao/tao);
    end
end
```

在x=20处分析，理论值为y = 40x – 400。经过模拟得到的值为y=39.6825x-368.253。
下图为得到的结果，实际上已经非常接近这个二次曲线在20这个位置的切线了。

![局部加权分析结果图](http://ord3iii9m.bkt.clouddn.com/image/github/mlearning%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0-%E5%B1%80%E9%83%A8%E5%8A%A0%E6%9D%83%E5%88%86%E6%9E%90%E7%BB%93%E6%9E%9C%E5%9B%BE.png)


## 2 回归分析相关概念
另外引申一个概念:

之前我们把房价与房屋大小认为是一个一维曲线。如上图，可以看出很多点被反映到曲线中。我们可以称其为欠拟合过程。与之对应的，如果使用高阶函数进行拟合，即假如样本有5个点，我们可以通过一个4阶函数的曲线进行完整拟合，但是这样的曲线往往并不是一个良好的房价与房屋大小的反映，这被称为过拟合。(该部分内容可以想见矩阵与数值分析)


