---
title: 机器学习-2.3-监督学习之通用线性模型
date: 2017-07-16 14:33:58
tags:
---

<script type="text/javascript" src="/Users/zcy/Desktop/study/git/MathJax-master/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

# Generalized Linear Models

回归分析中我们使用了高斯分布,分类问题服从伯努利分布。这两种方法都是第一个更通用的分布的特例，我们称其Generalized Linear Models (GLMs)

## 1 指数族分布

我们定义如下形式为指数族分布：

{% raw %}$$p(y;\eta ) = b(y)exp({\eta ^T}T(y) - a(\eta ))$$
{% endraw %}
其中,\\(\eta \\)是自然参数(natural parameter )，\\(T(y) \\)叫做充分统计量(sufficient statistic),一般\\(T(y)=y\\)。\\(a(\eta) \\)是log partition function 。\\({e^{ - a\left( \eta  \right)}}\\)在归整化很重要，这个参数确保\\(p(y;\eta )\\)归整为1。

## 2 指数族分布的特例

其中伯努利分布和高斯分布都是指数族分布的一个特例。

### 2.1 伯努利分布的指数族表达伯努利分布的指数表示:{% raw %}$$p(y,\varphi ) = {\varphi ^y}{(1 - \varphi )^{1 - y}} = \exp (y\log \frac{\varphi }{1 - \varphi } + \log (1 - \varphi ))$$
{% endraw %}

对照指数函数族的表达式，有\\(b(y) = 1\\),\\(T(y) = y\\),\\(\eta  = \log \frac{\varphi }{1 - \varphi}\\)等价于\\(\varphi=\frac{e^\eta}{e^\eta + 1}\\),\\(a(y) = \log \frac{1}{1 - \varphi } = \log ({e^\eta } + 1)\\)### 2.2 高斯分布的指数族表达高斯分布的指数表示:{% raw %}$$p(y;\mu ) = \frac{1}{{\sqrt {2\pi } }}\exp ( - \frac{1}{2}{(y - \mu )^2}) = \frac{1}{{\sqrt {2\pi } }}\exp ( - \frac{1}{2}{y^2})\exp (\mu y - \frac{1}{2}{\mu ^2})$$
{% endraw %}

对照指数函数族的表达式，有\\(b(y) = \frac{1}{\sqrt {2\pi }}\exp ( - \frac{1}{2}{y^2})\\),\\( \eta = \mu \\),\\(T(y) = y\\),\\(a(\eta ) = \frac{1}{2}{u^2} = \frac{1}{2}{\eta ^2}\\)。

## 3 指数函数组的三个假设

三个假设分别为:

* \\(y|x;\theta\\)服从指数分布
* 给定\\(x\\)的情况下，我们的目标是预测\\(T(y)\\)。在大多数的例子中，\\(T(y)=y\\),因此就意味着我们通过学习得到函数h, 使得\\(h(x)=E(y|x)\\), 意思就是给定x，得到函数h使得其与训练样本的数学期望相等。
* 自然参数\\(\theta\\)与输入\\(x\\)是线性关系。


对照上面的过程，我们可知伯努利分布和高斯分布都符合这三个假设

## 4 GLMs公式推导还是之前的问题，已知有\\(m\\)个样本，通过学习模拟正确的\\(h(x)\\),使得其能够有效的推测\\(y\\)。(这里我们假定了\\(T(y)=y\\))

得到最大释然函数:

{% raw %}$$l(\eta ) = \sum\limits_{i = 1}^m {b({y^i}){\text{ }}exp({\eta ^T}T({y^i}) - a(\eta ))} $$
{% endraw %}

然后为了求其最大值，我们对其取自然对数后求倒数。

{% raw %}$$\frac{{\partial \ln (l(\eta ))}}{{\partial \eta }} = \sum\limits _{i = 1}^m {\frac{{\partial (\ln b({y^i}) + {\eta ^T}T({y^i}) - a(\eta ))}}{{\partial \eta }}}  = \sum\limits _{i = 1}^m {(T({y^i})}  - \frac{{\partial a(\eta )}}{{\partial \eta }})$$
{% endraw %}

在我们之前指数族分布的定义3中，\\(\eta\\)是各个维度\\(x\\)的线性表达。有\\(\eta  = {\theta ^T}x\\)。所以，我们对每一个维度求偏导数，有：{% raw %}$$\frac{{\partial \ln (l(\eta ))}}{{\partial {\theta _j}}} = \frac{{\partial \ln (l(\eta ))}}{{\partial \eta }}\frac{{\partial \eta }}{{\partial {\theta _j}}} = \sum\limits _{i = 1}^m {(T({y^i})}  - \frac{{\partial a(\eta )}}{{\partial \eta }})x _j^i$${% endraw %}如果指数分布为伯努利分布，我们将式子带入其中，能够轻易的关于\\(\theta _j\\)的偏导数，且与之前推导的结果一致，具体过程不详细说了。高斯分布同理。